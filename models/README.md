# JiT Model Module

åŸºäº PyTorch Lightning çš„ JiT æ‰©æ•£æ¨¡å‹æ¨¡å—ã€‚

## ğŸ“¦ æ¨¡å—ç»„æˆ

### `modelmodule.py` - Lightning æ¨¡å‹æ¨¡å—

åŒ…å« `JiTLightningModule` ç±»ï¼Œå°è£…äº†è®­ç»ƒã€éªŒè¯å’Œå›¾åƒç”Ÿæˆçš„å®Œæ•´é€»è¾‘ã€‚

## ğŸ¯ æ ¸å¿ƒç±»ï¼šJiTLightningModule

### åŠŸèƒ½ç‰¹ç‚¹

- âœ… ç»§æ‰¿è‡ª `pl.LightningModule`
- âœ… å°è£…å®Œæ•´çš„æ‰©æ•£æ¨¡å‹è®­ç»ƒæµç¨‹
- âœ… æ”¯æŒ Classifier-free Guidance (CFG)
- âœ… å†…ç½® EMA å‚æ•°ç®¡ç†
- âœ… æ”¯æŒå¤šç§ ODE é‡‡æ ·æ–¹æ³•ï¼ˆEulerã€Heunï¼‰
- âœ… è‡ªåŠ¨ä¿å­˜å’ŒåŠ è½½è¶…å‚æ•°
- âœ… çµæ´»çš„ä¼˜åŒ–å™¨é…ç½®

### å‚æ•°è¯´æ˜

```python
JiTLightningModule(
    # æ¨¡å‹æ¶æ„å‚æ•°
    model_name: str = 'JiT-B/16',           # æ¨¡å‹åç§°
    img_size: int = 256,                     # å›¾åƒå°ºå¯¸
    num_classes: int = 1000,                 # ç±»åˆ«æ•°é‡
    attn_dropout: float = 0.0,               # æ³¨æ„åŠ› dropout
    proj_dropout: float = 0.0,               # æŠ•å½±å±‚ dropout
    
    # ä¼˜åŒ–å™¨å‚æ•°
    learning_rate: float = 1e-4,             # å­¦ä¹ ç‡
    weight_decay: float = 0.0,               # æƒé‡è¡°å‡
    
    # EMA å‚æ•°
    ema_decay1: float = 0.9999,              # ç¬¬ä¸€ä¸ª EMA è¡°å‡ç‡
    ema_decay2: float = 0.9996,              # ç¬¬äºŒä¸ª EMA è¡°å‡ç‡
    
    # æ‰©æ•£æ¨¡å‹å‚æ•°
    P_mean: float = -0.8,                    # æ—¶é—´æ­¥é‡‡æ ·å‡å€¼
    P_std: float = 0.8,                      # æ—¶é—´æ­¥é‡‡æ ·æ ‡å‡†å·®
    noise_scale: float = 1.0,                # å™ªå£°ç¼©æ”¾å› å­
    t_eps: float = 5e-2,                     # æ—¶é—´æ­¥æœ€å°å€¼
    label_drop_prob: float = 0.1,            # æ ‡ç­¾ä¸¢å¼ƒæ¦‚ç‡ï¼ˆCFGï¼‰
    
    # é‡‡æ ·å‚æ•°
    sampling_method: str = 'heun',           # é‡‡æ ·æ–¹æ³• ('euler' æˆ– 'heun')
    num_sampling_steps: int = 50,            # é‡‡æ ·æ­¥æ•°
    cfg_scale: float = 1.0,                  # CFG ç¼©æ”¾å› å­
    cfg_interval: tuple = (0.0, 1.0),        # CFG åº”ç”¨åŒºé—´
)
```

## ğŸ—ï¸ æ¨¡å‹æ¶æ„

### å¯ç”¨çš„æ¨¡å‹å˜ä½“

| æ¨¡å‹åç§° | è§„æ¨¡ | Patch Size | å‚æ•°é‡ | Hidden Size | Depth | Heads |
|---------|------|------------|--------|-------------|-------|-------|
| JiT-B/16 | Base | 16Ã—16 | ~100M | 768 | 12 | 12 |
| JiT-B/32 | Base | 32Ã—32 | ~100M | 768 | 12 | 12 |
| JiT-L/16 | Large | 16Ã—16 | ~300M | 1024 | 24 | 16 |
| JiT-L/32 | Large | 32Ã—32 | ~300M | 1024 | 24 | 16 |
| JiT-H/16 | Huge | 16Ã—16 | ~600M | 1280 | 32 | 16 |
| JiT-H/32 | Huge | 32Ã—32 | ~600M | 1280 | 32 | 16 |

### æ¨¡å‹ç»„ä»¶

```
JiTLightningModule
â”œâ”€â”€ net (JiT Transformer)
â”‚   â”œâ”€â”€ x_embedder (BottleneckPatchEmbed)
â”‚   â”œâ”€â”€ t_embedder (TimestepEmbedder)
â”‚   â”œâ”€â”€ y_embedder (LabelEmbedder)
â”‚   â”œâ”€â”€ blocks (JiTBlock Ã— N)
â”‚   â”‚   â”œâ”€â”€ attn (Attention + RoPE)
â”‚   â”‚   â”œâ”€â”€ mlp (SwiGLU FFN)
â”‚   â”‚   â””â”€â”€ adaLN (Adaptive Layer Norm)
â”‚   â””â”€â”€ final_layer (FinalLayer)
â”œâ”€â”€ ema_params1 (EMA parameters)
â””â”€â”€ ema_params2 (EMA parameters)
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³• 1: ç›´æ¥åˆ›å»ºæ¨¡å‹

```python
from models.modelmodule import JiTLightningModule

# åˆ›å»ºæ¨¡å‹
model = JiTLightningModule(
    model_name='JiT-B/16',
    img_size=256,
    num_classes=1000,
    learning_rate=1e-4,
    ema_decay1=0.9999,
    ema_decay2=0.9996,
    sampling_method='heun',
    num_sampling_steps=50,
    cfg_scale=2.9,
    cfg_interval=(0.1, 1.0),
)

# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
print(f"å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
print(f"è¶…å‚æ•°: {model.hparams}")
```

### æ–¹æ³• 2: ä¸ Lightning Trainer é…åˆ

```python
import lightning.pytorch as pl
from models.modelmodule import JiTLightningModule
from datas.datamodule import JiTDataModule

# åˆ›å»ºæ¨¡å‹å’Œæ•°æ®æ¨¡å—
model = JiTLightningModule(model_name='JiT-B/16', img_size=256)
datamodule = JiTDataModule(data_path='./data/imagenet', img_size=256)

# åˆ›å»º Trainer
trainer = pl.Trainer(
    max_epochs=600,
    accelerator='gpu',
    devices=8,
    strategy='ddp',
    precision='bf16-mixed',
)

# è®­ç»ƒ
trainer.fit(model, datamodule=datamodule)
```

### æ–¹æ³• 3: ä»å‚æ•°å¯¹è±¡åˆ›å»º

```python
from models.modelmodule import create_jit_lightning_module

# ä» argparse å‚æ•°åˆ›å»º
model = create_jit_lightning_module(args)
```

### æ–¹æ³• 4: åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

```python
# ä» checkpoint åŠ è½½
model = JiTLightningModule.load_from_checkpoint('checkpoints/last.ckpt')

# æŸ¥çœ‹ä¿å­˜çš„è¶…å‚æ•°
print(model.hparams)
```

## ğŸ”„ è®­ç»ƒæµç¨‹

### å‰å‘ä¼ æ’­

```python
def forward(self, x, t, y):
    """
    Args:
        x: è¾“å…¥å›¾åƒ [B, C, H, W]
        t: æ—¶é—´æ­¥ [B]
        y: ç±»åˆ«æ ‡ç­¾ [B]
    
    Returns:
        é¢„æµ‹çš„å›¾åƒ [B, C, H, W]
    """
    return self.net(x, t, y)
```

### è®­ç»ƒæ­¥éª¤

```python
def training_step(self, batch, batch_idx):
    """
    è®­ç»ƒæ­¥éª¤æµç¨‹:
    1. æ•°æ®é¢„å¤„ç† (å½’ä¸€åŒ–åˆ° [-1, 1])
    2. éšæœºä¸¢å¼ƒæ ‡ç­¾ (CFG)
    3. é‡‡æ ·æ—¶é—´æ­¥ (logit-normal åˆ†å¸ƒ)
    4. æ·»åŠ å™ªå£°
    5. æ¨¡å‹é¢„æµ‹
    6. è®¡ç®— L2 æŸå¤±
    7. è®°å½•æ—¥å¿—
    """
    images, labels = batch
    x = images.float() / 127.5 - 1.0
    
    labels_dropped = self.drop_labels(labels)
    t = self.sample_timestep(x.size(0), device=x.device)
    
    e = torch.randn_like(x) * self.noise_scale
    z = t * x + (1 - t) * e
    v = (x - z) / (1 - t).clamp_min(self.t_eps)
    
    x_pred = self(z, t.flatten(), labels_dropped)
    v_pred = (x_pred - z) / (1 - t).clamp_min(self.t_eps)
    
    loss = F.mse_loss(v_pred, v)
    self.log('train/loss', loss)
    
    return loss
```

### ä¼˜åŒ–å™¨é…ç½®

```python
def configure_optimizers(self):
    """
    é…ç½® AdamW ä¼˜åŒ–å™¨
    - ä¸º bias å’Œ norm å±‚è®¾ç½®é›¶æƒé‡è¡°å‡
    - ä½¿ç”¨ (0.9, 0.95) çš„ beta å€¼
    """
    optimizer = torch.optim.AdamW(
        [
            {'params': decay_params, 'weight_decay': self.weight_decay},
            {'params': no_decay_params, 'weight_decay': 0.0},
        ],
        lr=self.learning_rate,
        betas=(0.9, 0.95)
    )
    return optimizer
```

## ğŸ¨ å›¾åƒç”Ÿæˆ

### åŸºæœ¬ç”Ÿæˆ

```python
# åˆ›å»ºæ¨¡å‹
model = JiTLightningModule(...)
model.eval()
model.cuda()

# å‡†å¤‡æ ‡ç­¾
labels = torch.tensor([1, 2, 3, 4], device='cuda')  # ç±»åˆ« ID

# ç”Ÿæˆå›¾åƒ
with torch.no_grad():
    generated_images = model.generate(labels, use_ema=True)

# generated_images: [B, 3, H, W]ï¼ŒèŒƒå›´ [-1, 1]
```

### é«˜çº§ç”Ÿæˆé€‰é¡¹

```python
# ä½¿ç”¨ä¸åŒçš„é‡‡æ ·æ–¹æ³•
model.sampling_method = 'euler'  # æˆ– 'heun'
model.num_sampling_steps = 50    # é‡‡æ ·æ­¥æ•°

# è°ƒæ•´ CFG å¼ºåº¦
model.cfg_scale = 2.9           # å¼•å¯¼å¼ºåº¦
model.cfg_interval = (0.1, 1.0) # åº”ç”¨åŒºé—´

# ç”Ÿæˆ
images = model.generate(labels)

# è½¬æ¢ä¸º uint8 å›¾åƒ
images_uint8 = ((images + 1.0) * 127.5).clamp(0, 255).byte()
```

### ODE é‡‡æ ·æ–¹æ³•

#### Euler æ–¹æ³•ï¼ˆä¸€é˜¶ï¼‰
```python
def _euler_step(z, t, t_next, labels):
    """
    æ¬§æ‹‰æ–¹æ³•ï¼š
    z_next = z + (t_next - t) * v_pred
    """
    v_pred = _forward_sample(z, t, labels)
    z_next = z + (t_next - t) * v_pred
    return z_next
```

#### Heun æ–¹æ³•ï¼ˆäºŒé˜¶ï¼‰
```python
def _heun_step(z, t, t_next, labels):
    """
    Heun æ–¹æ³•ï¼ˆæ”¹è¿›çš„æ¬§æ‹‰æ–¹æ³•ï¼‰ï¼š
    1. é¢„æµ‹ z_next (Euler)
    2. è®¡ç®—ä¿®æ­£åçš„é€Ÿåº¦
    3. ä½¿ç”¨å¹³å‡é€Ÿåº¦æ›´æ–°
    """
    v_pred_t = _forward_sample(z, t, labels)
    z_next_euler = z + (t_next - t) * v_pred_t
    v_pred_t_next = _forward_sample(z_next_euler, t_next, labels)
    v_pred = 0.5 * (v_pred_t + v_pred_t_next)
    z_next = z + (t_next - t) * v_pred
    return z_next
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰è®­ç»ƒæ­¥éª¤

```python
class CustomJiTModule(JiTLightningModule):
    def training_step(self, batch, batch_idx):
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        loss = super().training_step(batch, batch_idx)
        
        # æ·»åŠ è‡ªå®šä¹‰é€»è¾‘
        if batch_idx % 100 == 0:
            # è®°å½•é¢å¤–çš„æŒ‡æ ‡
            self.log('custom_metric', some_value)
        
        return loss
```

### è‡ªå®šä¹‰é‡‡æ ·æ–¹æ³•

```python
class CustomJiTModule(JiTLightningModule):
    @torch.no_grad()
    def _custom_sampler(self, z, t, t_next, labels):
        """è‡ªå®šä¹‰ ODE æ±‚è§£å™¨"""
        # å®ç°ä½ çš„é‡‡æ ·é€»è¾‘
        v_pred = self._forward_sample(z, t, labels)
        z_next = z + (t_next - t) * v_pred * some_factor
        return z_next
    
    def generate(self, labels, use_ema=True):
        # ä½¿ç”¨è‡ªå®šä¹‰é‡‡æ ·å™¨
        for i in range(self.num_sampling_steps):
            z = self._custom_sampler(z, t, t_next, labels)
        return z
```

### å¤š EMA ç‰ˆæœ¬

```python
# æ¨¡å‹å†…ç½®ä¸¤ä¸ª EMA ç‰ˆæœ¬
# EMA1: decay=0.9999 (æ›´å¹³æ»‘)
# EMA2: decay=0.9996 (æ›´å¿«é€‚åº”)

# åœ¨ callbacks ä¸­å¯ä»¥é€‰æ‹©ä½¿ç”¨å“ªä¸ªç‰ˆæœ¬
ema_callback.load_ema_to_model(model, ema_version=1)  # ä½¿ç”¨ EMA1
# æˆ–
ema_callback.load_ema_to_model(model, ema_version=2)  # ä½¿ç”¨ EMA2
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### æ··åˆç²¾åº¦è®­ç»ƒ

```python
trainer = pl.Trainer(
    precision='bf16-mixed',  # ä½¿ç”¨ BFloat16 æ··åˆç²¾åº¦
    # æˆ–
    precision='16-mixed',    # ä½¿ç”¨ Float16 æ··åˆç²¾åº¦
)
```

### æ¢¯åº¦ç´¯ç§¯

```python
trainer = pl.Trainer(
    accumulate_grad_batches=2,  # ç´¯ç§¯ 2 ä¸ªæ‰¹æ¬¡çš„æ¢¯åº¦
)
```

### æ¢¯åº¦è£å‰ª

```python
trainer = pl.Trainer(
    gradient_clip_val=1.0,      # æ¢¯åº¦è£å‰ªé˜ˆå€¼
)
```

### ç¼–è¯‘ä¼˜åŒ–ï¼ˆPyTorch 2.0+ï¼‰

```python
# æ¨¡å‹ä¸­çš„æŸäº›æ–¹æ³•å·²ç»ä½¿ç”¨ @torch.compile è£…é¥°
# ä¾‹å¦‚ JiTBlock.forward() å’Œ FinalLayer.forward()
# è¿™ä¼šè‡ªåŠ¨è¿›è¡Œ JIT ç¼–è¯‘ä¼˜åŒ–
```

## ğŸ“ˆ ç›‘æ§å’Œæ—¥å¿—

### è‡ªåŠ¨è®°å½•çš„æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | è®°å½•é¢‘ç‡ |
|------|------|---------|
| `train/loss` | è®­ç»ƒæŸå¤± | æ¯ä¸ª step |
| `train/lr` | å­¦ä¹ ç‡ | æ¯ 100 steps |
| `val/loss` | éªŒè¯æŸå¤± | æ¯ä¸ª epoch |
| `eval/fid` | FID åˆ†æ•° | è¯„ä¼°æ—¶ |
| `eval/is` | Inception Score | è¯„ä¼°æ—¶ |

### è‡ªå®šä¹‰æ—¥å¿—

```python
def training_step(self, batch, batch_idx):
    loss = ...
    
    # è®°å½•åˆ° TensorBoard
    self.log('train/loss', loss, prog_bar=True)
    self.log('train/custom_metric', value, on_step=True, on_epoch=True)
    
    return loss
```

## âš™ï¸ é…ç½®å»ºè®®

### JiT-B/16 @ 256Ã—256

```python
model = JiTLightningModule(
    model_name='JiT-B/16',
    img_size=256,
    learning_rate=5e-5 * 8 / 2,  # æ ¹æ® GPU æ•°é‡ç¼©æ”¾
    proj_dropout=0.0,
    P_mean=-0.8,
    P_std=0.8,
    noise_scale=1.0,
    cfg_scale=2.9,
)
```

### JiT-L/16 @ 256Ã—256

```python
model = JiTLightningModule(
    model_name='JiT-L/16',
    img_size=256,
    learning_rate=5e-5 * 8 / 2,
    proj_dropout=0.0,
    cfg_scale=2.4,
)
```

### JiT-H/16 @ 256Ã—256

```python
model = JiTLightningModule(
    model_name='JiT-H/16',
    img_size=256,
    learning_rate=5e-5 * 8 / 2,
    proj_dropout=0.2,  # æ›´å¤§çš„æ¨¡å‹éœ€è¦æ›´å¤šæ­£åˆ™åŒ–
    cfg_scale=2.2,
)
```

### 512Ã—512 å›¾åƒ

```python
model = JiTLightningModule(
    model_name='JiT-B/32',  # ä½¿ç”¨ 32Ã—32 patch
    img_size=512,
    noise_scale=2.0,        # æ›´å¤§çš„å™ªå£°å°ºåº¦
    cfg_scale=2.9,
)
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å›¾åƒå½’ä¸€åŒ–** - è®­ç»ƒæ—¶å›¾åƒè‡ªåŠ¨å½’ä¸€åŒ–åˆ° [-1, 1]ï¼Œç”Ÿæˆæ—¶è¾“å‡ºä¹Ÿæ˜¯ [-1, 1]
2. **EMA å‚æ•°** - EMA å‚æ•°ç”± callbacks ç®¡ç†ï¼Œä¸è¦æ‰‹åŠ¨æ›´æ–°
3. **è¶…å‚æ•°ä¿å­˜** - `save_hyperparameters()` ä¼šè‡ªåŠ¨ä¿å­˜æ‰€æœ‰åˆå§‹åŒ–å‚æ•°åˆ° checkpoint
4. **åˆ†å¸ƒå¼è®­ç»ƒ** - ä½¿ç”¨ DDP æ—¶ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨åŒæ­¥æ¢¯åº¦
5. **å†…å­˜ä½¿ç”¨** - Huge æ¨¡å‹éœ€è¦è‡³å°‘ 80GB GPU å†…å­˜ï¼ˆä½¿ç”¨æ··åˆç²¾åº¦ï¼‰

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1: è®­ç»ƒæŸå¤±ä¸æ”¶æ•›
**å¯èƒ½åŸå› **: å­¦ä¹ ç‡è¿‡å¤§æˆ–è¿‡å°  
**è§£å†³æ–¹æ¡ˆ**: è°ƒæ•´ `learning_rate`ï¼Œå»ºè®®èŒƒå›´ [1e-5, 1e-4]

### é—®é¢˜ 2: ç”Ÿæˆå›¾åƒè´¨é‡å·®
**å¯èƒ½åŸå› **: 
- æœªä½¿ç”¨ EMA å‚æ•°
- CFG å¼ºåº¦ä¸åˆé€‚
- é‡‡æ ·æ­¥æ•°è¿‡å°‘

**è§£å†³æ–¹æ¡ˆ**:
```python
# ç¡®ä¿ä½¿ç”¨ EMA
images = model.generate(labels, use_ema=True)

# è°ƒæ•´ CFG
model.cfg_scale = 2.9
model.cfg_interval = (0.1, 1.0)

# å¢åŠ é‡‡æ ·æ­¥æ•°
model.num_sampling_steps = 100
```

### é—®é¢˜ 3: å†…å­˜æº¢å‡º
**è§£å†³æ–¹æ¡ˆ**:
- å‡å° batch_size
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
- é€‰æ‹©è¾ƒå°çš„æ¨¡å‹ï¼ˆB ä»£æ›¿ L æˆ– Hï¼‰

### é—®é¢˜ 4: åˆ†å¸ƒå¼è®­ç»ƒé€Ÿåº¦æ…¢
**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨ `strategy='ddp'` è€Œä¸æ˜¯ 'ddp_spawn'
- ç¡®ä¿ `pin_memory=True` in DataModule
- å¢åŠ  `num_workers`

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [PyTorch Lightning LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html)
- [JiT è®ºæ–‡](https://arxiv.org/abs/2511.13720)
- [Diffusion Models](https://arxiv.org/abs/2006.11239)

## ğŸ”— ä¸å…¶ä»–æ¨¡å—çš„é›†æˆ

### ä¸æ•°æ®æ¨¡å—é›†æˆ

```python
from models.modelmodule import JiTLightningModule
from datas.datamodule import JiTDataModule

# ç¡®ä¿å›¾åƒå°ºå¯¸åŒ¹é…
img_size = 256
model = JiTLightningModule(img_size=img_size)
datamodule = JiTDataModule(img_size=img_size)
```

### ä¸ Callbacks é›†æˆ

```python
from models.modelmodule import JiTLightningModule
from callbacks import create_default_callbacks

model = JiTLightningModule(...)
callbacks = create_default_callbacks(
    ema_decay1=model.ema_decay1,
    ema_decay2=model.ema_decay2,
    img_size=model.img_size,
)
```

## ğŸ“„ è®¸å¯è¯

ä¸ä¸»é¡¹ç›®ç›¸åŒçš„è®¸å¯è¯ã€‚

